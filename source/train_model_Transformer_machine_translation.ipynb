{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj8DYAkO2DZm"
      },
      "source": [
        "## Đặng Nguyễn Quang Huy Intern AI Engineer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yweJ1B6I2DZo"
      },
      "source": [
        "## 1.Import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 643,
      "metadata": {
        "id": "tQBRxYR-2DZo"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIb3m9IS2VMt",
        "outputId": "cc7c19d7-e3a7-4958-9a0b-85fb26f31c97"
      },
      "execution_count": 644,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voApd2ik2DZp"
      },
      "source": [
        "## 2. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2ejkwD72DZp"
      },
      "source": [
        "> Get data origin from url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 645,
      "metadata": {
        "id": "hX7712JM2DZq"
      },
      "outputs": [],
      "source": [
        "url = f\"https://datasets-server.huggingface.co/rows?dataset=Helsinki-NLP%2Fopus_books&config=en-hu&split=train&offset=0&length=100\"\n",
        "\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    data = response.json()\n",
        "    rows = data.get('rows', [])\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv('/content/drive/MyDrive/Machine_Translation/data/data_origin.csv', index=False, encoding='utf-8')\n",
        "else:\n",
        "    print(f\"Failed to retrieve data: {response.status_code}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx0aSorH2DZq"
      },
      "source": [
        "> Read data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 646,
      "metadata": {
        "id": "DCxpnqg42DZq"
      },
      "outputs": [],
      "source": [
        "# Đọc dữ liệu từ CSV\n",
        "df = pd.read_csv('/content/drive/MyDrive/Machine_Translation/data/data_origin.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57m5ky0A2DZr"
      },
      "source": [
        "> Extract pairs of translated sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 647,
      "metadata": {
        "id": "ZhEj1Ias2DZr"
      },
      "outputs": [],
      "source": [
        "pairs = []\n",
        "for row in df.itertuples(index=False):\n",
        "    translation = eval(row.row)['translation']\n",
        "    pairs.append((translation['en'], translation['hu']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yutOETD2DZr"
      },
      "source": [
        "> Create tokenizer for English and Hungarian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 648,
      "metadata": {
        "id": "mEYgNFzo2DZr"
      },
      "outputs": [],
      "source": [
        "tokenizer_en = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenizer_hu = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_lTW3762DZr"
      },
      "source": [
        ">  Create dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 649,
      "metadata": {
        "id": "ReFGDx5n2DZr"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, pairs, tokenizer_en, tokenizer_hu, max_length=128):\n",
        "        self.pairs = pairs\n",
        "        self.tokenizer_en = tokenizer_en\n",
        "        self.tokenizer_hu = tokenizer_hu\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        en_text, hu_text = self.pairs[idx]\n",
        "        en_encoding = self.tokenizer_en.encode_plus(\n",
        "            en_text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "        hu_encoding = self.tokenizer_hu.encode_plus(\n",
        "            hu_text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        en_input_ids = en_encoding['input_ids'].squeeze(0)\n",
        "        hu_input_ids = hu_encoding['input_ids'].squeeze(0)\n",
        "\n",
        "        min_len = min(en_input_ids.shape[0], hu_input_ids.shape[0])\n",
        "\n",
        "        return en_input_ids[:min_len], hu_input_ids[:min_len]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4cK5T842DZr"
      },
      "source": [
        "> Create training set, test set, validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 650,
      "metadata": {
        "id": "-Hpe3oOs2DZr"
      },
      "outputs": [],
      "source": [
        "dataset = TranslationDataset(pairs, tokenizer_en, tokenizer_hu)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhBSpXIX2DZs"
      },
      "source": [
        "## 3.Build a Transformer model for machine translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn9SITNx2DZs"
      },
      "source": [
        "> Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 651,
      "metadata": {
        "id": "5HZzILEK2DZs"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, nhead=8, num_encoder_layers=4, num_decoder_layers=4 ,dim_feedforward=2460, dropout=0.1):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward, dropout=dropout\n",
        "        )\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src = src.permute(1, 0)\n",
        "        tgt = tgt.permute(1, 0)\n",
        "        src_emb = self.src_embedding(src)\n",
        "        tgt_emb = self.tgt_embedding(tgt)\n",
        "        output = self.transformer(src_emb, tgt_emb)\n",
        "        output = output.permute(1, 0, 2)\n",
        "        return self.fc_out(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 652,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90ESJOj_2DZs",
        "outputId": "f1d9308c-4584-4469-e50a-84c26cb8924f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "src_vocab_size = len(tokenizer_en)\n",
        "tgt_vocab_size = len(tokenizer_hu)\n",
        "model = TransformerModel(src_vocab_size, tgt_vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-3D_NzJ2DZs"
      },
      "source": [
        "> Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 653,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgLUl0TO2DZs",
        "outputId": "69a5f298-8aa3-452e-f7f9-373aae51a5fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 11.0233, Val Loss: 10.5503\n",
            "Epoch 2, Train Loss: 10.1820, Val Loss: 10.0860\n",
            "Epoch 3, Train Loss: 9.5491, Val Loss: 9.5903\n",
            "Epoch 4, Train Loss: 8.9040, Val Loss: 9.1214\n",
            "Epoch 5, Train Loss: 8.2605, Val Loss: 8.6662\n",
            "Epoch 6, Train Loss: 7.6670, Val Loss: 8.2644\n",
            "Epoch 7, Train Loss: 7.1191, Val Loss: 7.9902\n",
            "Epoch 8, Train Loss: 6.6529, Val Loss: 7.7611\n",
            "Epoch 9, Train Loss: 6.2399, Val Loss: 7.6087\n",
            "Epoch 10, Train Loss: 5.8528, Val Loss: 7.4931\n",
            "Epoch 11, Train Loss: 5.4618, Val Loss: 7.4475\n",
            "Epoch 12, Train Loss: 5.1071, Val Loss: 7.3756\n",
            "Epoch 13, Train Loss: 4.7454, Val Loss: 7.3089\n",
            "Epoch 14, Train Loss: 4.3842, Val Loss: 7.2524\n",
            "Epoch 15, Train Loss: 4.0703, Val Loss: 7.2116\n",
            "Epoch 16, Train Loss: 3.7903, Val Loss: 7.2143\n",
            "Epoch 17, Train Loss: 3.4860, Val Loss: 7.1792\n",
            "Epoch 18, Train Loss: 3.2463, Val Loss: 7.1782\n",
            "Epoch 19, Train Loss: 2.9803, Val Loss: 7.1619\n",
            "Epoch 20, Train Loss: 2.7732, Val Loss: 7.1612\n",
            "Epoch 21, Train Loss: 2.5353, Val Loss: 7.1719\n",
            "Epoch 22, Train Loss: 2.3715, Val Loss: 7.1522\n",
            "Epoch 23, Train Loss: 2.1800, Val Loss: 7.1622\n",
            "Epoch 24, Train Loss: 2.0106, Val Loss: 7.1633\n",
            "Epoch 25, Train Loss: 1.8277, Val Loss: 7.1572\n",
            "Epoch 26, Train Loss: 1.6887, Val Loss: 7.2136\n",
            "Epoch 27, Train Loss: 1.5518, Val Loss: 7.2163\n",
            "Epoch 28, Train Loss: 1.4328, Val Loss: 7.2125\n",
            "Epoch 29, Train Loss: 1.2980, Val Loss: 7.2191\n",
            "Epoch 30, Train Loss: 1.1973, Val Loss: 7.2493\n",
            "Epoch 31, Train Loss: 1.1007, Val Loss: 7.2763\n",
            "Epoch 32, Train Loss: 1.0002, Val Loss: 7.3318\n",
            "Validation loss has not improved for 10 epochs. Early stopping...\n"
          ]
        }
      ],
      "source": [
        "# Move model and data to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer_hu.pad_token_id)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Training function for each epoch\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for src, tgt in dataloader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "        # Ensure the batch sizes of src and tgt are equal\n",
        "        if src.size(0) != tgt.size(0):\n",
        "            raise RuntimeError(\"The batch sizes of src and tgt must be equal\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt[:, :-1])\n",
        "        output = output.view(-1, output.shape[-1])\n",
        "        tgt = tgt[:, 1:].contiguous().view(-1)\n",
        "        loss = criterion(output, tgt)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "# Evaluation function for the model on the validation set\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in dataloader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            output = model(src, tgt[:, :-1])\n",
        "            output = output.view(-1, output.shape[-1])\n",
        "            tgt = tgt[:, 1:].contiguous().view(-1)\n",
        "            loss = criterion(output, tgt)\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "# Train the model\n",
        "n_epochs = 100\n",
        "best_val_loss = float('inf')\n",
        "patience = 10\n",
        "counter = 0\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    # Check if validation loss has improved\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "    # If validation loss hasn't improved for 'patience' epochs, stop training\n",
        "    if counter >= patience:\n",
        "        print(f'Validation loss has not improved for {patience} epochs. Early stopping...')\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Evaluate on test set"
      ],
      "metadata": {
        "id": "ZDCykvle8_6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_test_loss = 0\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for src, tgt in test_loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        output = model(src, tgt[:, :-1])\n",
        "        output = output.view(-1, output.shape[-1])\n",
        "        tgt = tgt[:, 1:].contiguous().view(-1)\n",
        "        loss = criterion(output, tgt)\n",
        "        total_test_loss += loss.item()\n",
        "\n",
        "avg_test_loss = total_test_loss / len(test_loader)\n",
        "print(f'Average Test Loss: {avg_test_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gT-VUtK_3DaI",
        "outputId": "37bdd3bf-120f-496e-f2bf-9ebd0496df4d"
      },
      "execution_count": 654,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Test Loss: 6.6402\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}